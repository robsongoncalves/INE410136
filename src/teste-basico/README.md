# Mini-tutorial: HRM-style (sem CoT) para **extraÃ§Ã£o de metadados de texto**

Este exemplo mostra como adaptar o estilo HRM (raciocÃ­nio **latente**, sem Chain-of-Thought textual)
para **classificar metadados** a partir de textos curtos (toy dataset). O modelo nÃ£o imprime passos
intermediÃ¡rios; ele somente **emite as classes finais**: `autor`, `tipo_documento`, `ano`.

> Objetivo didÃ¡tico: demonstrar a ideia em NLP, usando PyTorch puro, tokenizaÃ§Ã£o simples e
> dois mÃ³dulos recorrentes (H e L) com iteraÃ§Ãµes internas.

---

## ğŸš€ Como rodar

```bash
pip install -r requirements.txt
python train_text_metadata.py --epochs 5 --train 3000 --val 600 --cycles 3 --steps 3
```

ParÃ¢metros Ãºteis:
- `--epochs`: Ã©pocas de treino (default 5)
- `--train`, `--val`: nÂº de exemplos sintÃ©ticos de treino/val (default 3000/600)
- `--cycles`: nÂº de ciclos (H)
- `--steps`: nÂº de passos do mÃ³dulo L por ciclo (L)
- `--device`: `cpu` ou `cuda`

SaÃ­da: accuracy por campo (autor, tipo, ano) e **triplet-accuracy** (acertar os 3 juntos).

---

## ğŸ§  Ideia do HRM-style (texto)

- **Embedding** de tokens (vocabulÃ¡rio gerado do dataset sintÃ©tico).
- **MÃ³dulo L (rÃ¡pido)** roda `T` passos por ciclo ajustando representaÃ§Ãµes locais.
- **MÃ³dulo H (lento)** atualiza ao final de cada ciclo e injeta contexto global.
- **Pooling**: usamos o vetor do token `[CLS]` do H para prever as 3 cabeÃ§as de saÃ­da.
- **Sem CoT**: o modelo faz as iteraÃ§Ãµes **internamente** e retorna sÃ³ as classes finais.

---

## ğŸ“ ObservaÃ§Ãµes

- Ã‰ um *toy example* â€” simples e legÃ­vel. Para dados reais, troque a tokenizaÃ§Ã£o,
  aumente embedding/d_model e ajuste o gerador de dados.
- VocÃª pode portar os mÃ³dulos H/L para GRU/MLP recorrente facilmente.

Bom estudo! ğŸ™‚
